{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732d5aa1",
   "metadata": {},
   "source": [
    "# Machine Learning Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4db410",
   "metadata": {},
   "source": [
    "In this task we need to classificate different texts(tweets) from beeing fake or real. We will use different techniques to clean the texts from irrelevant information. Then we will use BERTopic for topic modelling using a pipeline from different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feab46",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here we load the xlsx files that we will use and take a look about its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c92ec486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "   id                                              tweet label\n",
      "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
      "1   2  States reported 1121 deaths a small rise from ...  real\n",
      "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
      "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
      "4   5  Populous states can generate large case counts...  real\n",
      "\n",
      "Validation Data:\n",
      "   id                                              tweet label\n",
      "0   1  Chinese converting to Islam after realising th...  fake\n",
      "1   2  11 out of 13 people (from the Diamond Princess...  fake\n",
      "2   3  COVID-19 Is Caused By A Bacterium, Not Virus A...  fake\n",
      "3   4  Mike Pence in RNC speech praises Donald Trump’...  fake\n",
      "4   5  6/10 Sky's @EdConwaySky explains the latest #C...  real\n",
      "\n",
      "Test Data:\n",
      "   id                                              tweet\n",
      "0   1  Our daily update is published. States reported...\n",
      "1   2             Alfalfa is the only cure for COVID-19.\n",
      "2   3  President Trump Asked What He Would Do If He W...\n",
      "3   4  States reported 630 deaths. We are still seein...\n",
      "4   5  This is the sixth time a global health emergen...\n",
      "\n",
      "Labeled Test Data:\n",
      "   id                                              tweet label\n",
      "0   1  Our daily update is published. States reported...  real\n",
      "1   2             Alfalfa is the only cure for COVID-19.  fake\n",
      "2   3  President Trump Asked What He Would Do If He W...  fake\n",
      "3   4  States reported 630 deaths. We are still seein...  real\n",
      "4   5  This is the sixth time a global health emergen...  real\n"
     ]
    }
   ],
   "source": [
    "# Load the data from a Excel file and print the first 5 rows\n",
    "import pandas as pd\n",
    "train = pd.read_excel('data/Constraint_English_Train.xlsx')\n",
    "test = pd.read_excel('data/Constraint_English_Test.xlsx')\n",
    "val = pd.read_excel('data/Constraint_English_Val.xlsx')\n",
    "test_labeled = pd.read_excel('data/english_test_with_labels.xlsx')\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(train.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test.head())\n",
    "print(\"\\nLabeled Test Data:\")\n",
    "print(test_labeled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb5dc2",
   "metadata": {},
   "source": [
    "We import important modules that we will use to filter our data and tokenize it. We will use Spacy because it filter the words better that WordNet from NLTK thaks to its context-aware linguistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b473fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f6327",
   "metadata": {},
   "source": [
    "## Cleanning funcions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b8521",
   "metadata": {},
   "source": [
    "We implement two different funcions, the first one will remove URL's, numbers and extra spaces and will return us the original text with just simple words.\n",
    "\n",
    "The second funcions will use nlp from spacy to tokenize,remove stopwords, rewrite its lemma in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b4e74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Basic text cleaning: lowercase, remove URLs, non-letters and extra spaces.\"\"\"\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)         # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)                # keep only letters and spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()            # normalise spaces\n",
    "    return text\n",
    "\n",
    "# Tokenisation and stopword removal with spaCy\n",
    "def spacy_preprocess_with_lemmas(text):\n",
    "    \"\"\"Preprocessing with lemmatization\"\"\"\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in doc \n",
    "              if not token.is_stop and len(token.lemma_) > 2] # keep non-stopword lemmas longer than 2 characters(avoid lemmas like be, in,...)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273b5bc",
   "metadata": {},
   "source": [
    "This 3rd filter is used to eliminate words that doesnt exist like IndiaFightsCorona (as an unique word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e43fe952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordfreq\n",
      "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ftfy>=6.1 (from wordfreq)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: langcodes>=3.0 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (3.5.0)\n",
      "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
      "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (1.1.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (2025.9.18)\n",
      "Requirement already satisfied: wcwidth in /home/alberto/mi_entorno/lib/python3.12/site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.3.1)\n",
      "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: locate, ftfy, wordfreq\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [wordfreq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ftfy-6.3.1 locate-1.1.1 wordfreq-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wordfreq\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "def filter_real_words(tokens):\n",
    "\n",
    "    return [t for t in tokens if zipf_frequency(t, \"en\") > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0eba04",
   "metadata": {},
   "source": [
    "With all this filters and preprocessing we save the results in different columns to compare them in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05823bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_tweet'] = train['tweet'].apply(clean_text)\n",
    "train['tweet_tokens'] = train['cleaned_tweet'].apply(spacy_preprocess_with_lemmas)\n",
    "train['tweet_tokens'] = train['tweet_tokens'].apply(filter_real_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26f8f982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Tweet</th>\n",
       "      <th>Cleaned_Tweet</th>\n",
       "      <th>Tokens lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>The CDC currently reports deaths In general th...</td>\n",
       "      <td>[cdc, currently, report, death, general, discr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>States reported deaths a small rise from last ...</td>\n",
       "      <td>[state, report, death, small, rise, tuesday, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>Politically Correct Woman Almost Uses Pandemic...</td>\n",
       "      <td>[politically, correct, woman, use, pandemic, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>IndiaFightsCorona We have COVID testing labora...</td>\n",
       "      <td>[covid, testing, laboratory, india, august, test]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>[populous, state, generate, large, case, count...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Original_Tweet  ...                                      Tokens lemmas\n",
       "0  The CDC currently reports 99031 deaths. In gen...  ...  [cdc, currently, report, death, general, discr...\n",
       "1  States reported 1121 deaths a small rise from ...  ...  [state, report, death, small, rise, tuesday, s...\n",
       "2  Politically Correct Woman (Almost) Uses Pandem...  ...  [politically, correct, woman, use, pandemic, e...\n",
       "3  #IndiaFightsCorona: We have 1524 #COVID testin...  ...  [covid, testing, laboratory, india, august, test]\n",
       "4  Populous states can generate large case counts...  ...  [populous, state, generate, large, case, count...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_df = pd.DataFrame({\n",
    "    'Original_Tweet': train['tweet'],\n",
    "    'Cleaned_Tweet': train['cleaned_tweet'],\n",
    "    'Tokens lemmas': train['tweet_tokens']\n",
    "})\n",
    "comparative_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
