{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732d5aa1",
   "metadata": {},
   "source": [
    "# Machine Learning Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4db410",
   "metadata": {},
   "source": [
    "In this task we need to classificate different texts(tweets) from beeing fake or real. We will use different techniques to clean the texts from irrelevant information. Then we will use BERTopic for topic modelling using a pipeline from different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feab46",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here we load the xlsx files that we will use and take a look about its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c92ec486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "   id                                              tweet label\n",
      "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
      "1   2  States reported 1121 deaths a small rise from ...  real\n",
      "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
      "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
      "4   5  Populous states can generate large case counts...  real\n",
      "\n",
      "Validation Data:\n",
      "   id                                              tweet label\n",
      "0   1  Chinese converting to Islam after realising th...  fake\n",
      "1   2  11 out of 13 people (from the Diamond Princess...  fake\n",
      "2   3  COVID-19 Is Caused By A Bacterium, Not Virus A...  fake\n",
      "3   4  Mike Pence in RNC speech praises Donald Trumpâ€™...  fake\n",
      "4   5  6/10 Sky's @EdConwaySky explains the latest #C...  real\n",
      "\n",
      "Test Data:\n",
      "   id                                              tweet\n",
      "0   1  Our daily update is published. States reported...\n",
      "1   2             Alfalfa is the only cure for COVID-19.\n",
      "2   3  President Trump Asked What He Would Do If He W...\n",
      "3   4  States reported 630 deaths. We are still seein...\n",
      "4   5  This is the sixth time a global health emergen...\n",
      "\n",
      "Labeled Test Data:\n",
      "   id                                              tweet label\n",
      "0   1  Our daily update is published. States reported...  real\n",
      "1   2             Alfalfa is the only cure for COVID-19.  fake\n",
      "2   3  President Trump Asked What He Would Do If He W...  fake\n",
      "3   4  States reported 630 deaths. We are still seein...  real\n",
      "4   5  This is the sixth time a global health emergen...  real\n"
     ]
    }
   ],
   "source": [
    "# Load the data from a Excel file and print the first 5 rows\n",
    "import pandas as pd\n",
    "train = pd.read_excel('data/Constraint_English_Train.xlsx')\n",
    "test = pd.read_excel('data/Constraint_English_Test.xlsx')\n",
    "val = pd.read_excel('data/Constraint_English_Val.xlsx')\n",
    "test_labeled = pd.read_excel('data/english_test_with_labels.xlsx')\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print(train.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test.head())\n",
    "print(\"\\nLabeled Test Data:\")\n",
    "print(test_labeled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb5dc2",
   "metadata": {},
   "source": [
    "We import important modules that we will use to filter our data and tokenize it. We will use Spacy because it filter the words better that WordNet from NLTK thaks to its context-aware linguistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b473fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f6327",
   "metadata": {},
   "source": [
    "## Cleanning funcions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b8521",
   "metadata": {},
   "source": [
    "We implement two different funcions, the first one will remove URL's, numbers and extra spaces and will return us the original text with just simple words.\n",
    "\n",
    "The second funcions will use nlp from spacy to tokenize,remove stopwords, rewrite its lemma in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b4e74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Basic text cleaning: lowercase, remove URLs, non-letters and extra spaces.\"\"\"\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)         # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)                # keep only letters and spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()            # normalise spaces\n",
    "    return text\n",
    "\n",
    "# Tokenisation and stopword removal with spaCy\n",
    "def spacy_preprocess_with_lemmas(text):\n",
    "    \"\"\"Preprocessing with lemmatization\"\"\"\n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    lemmas = [token.lemma_ for token in doc \n",
    "              if not token.is_stop and len(token.lemma_) > 2] # keep non-stopword lemmas longer than 2 characters(avoid lemmas like be, in,...)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273b5bc",
   "metadata": {},
   "source": [
    "This 3rd filter is used to eliminate words that doesnt exist like IndiaFightsCorona (as an unique word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43fe952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordfreq in /home/alberto/mi_entorno/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: ftfy>=6.1 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (6.3.1)\n",
      "Requirement already satisfied: langcodes>=3.0 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (3.5.0)\n",
      "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (1.1.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (1.1.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from wordfreq) (2025.9.18)\n",
      "Requirement already satisfied: wcwidth in /home/alberto/mi_entorno/lib/python3.12/site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/alberto/mi_entorno/lib/python3.12/site-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wordfreq\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "def filter_real_words(tokens):\n",
    "\n",
    "    return [t for t in tokens if zipf_frequency(t, \"en\") > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b925525",
   "metadata": {},
   "source": [
    "In this case we use a function to count the different kind of words that appears in the tweets and later we will divide each type of word in different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa83497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_spacy_pos_counts(text):\n",
    "    doc = nlp(text)\n",
    "    counts = Counter([token.pos_ for token in doc])\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0eba04",
   "metadata": {},
   "source": [
    "With all this filters and preprocessing we save the results in different columns to compare them in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05823bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cleaned_tweet'] = train['tweet'].apply(clean_text)\n",
    "train['tweet_tokens'] = train['cleaned_tweet'].apply(spacy_preprocess_with_lemmas)\n",
    "train['tweet_tokens'] = train['tweet_tokens'].apply(filter_real_words)\n",
    "train[\"spacy_pos_counts\"] = train[\"cleaned_tweet\"].apply(get_spacy_pos_counts)\n",
    "\n",
    "pos_tags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"DET\", \"ADP\", \"CONJ\"]\n",
    "for tag in pos_tags:\n",
    "    train[tag + \"_spacy\"] = train[\"spacy_pos_counts\"].apply(lambda x: x.get(tag, 0))\n",
    "\n",
    "train[\"total_spacy\"] = train[[t + \"_spacy\" for t in pos_tags]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26f8f982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>spacy_pos_counts</th>\n",
       "      <th>NOUN_spacy</th>\n",
       "      <th>VERB_spacy</th>\n",
       "      <th>ADJ_spacy</th>\n",
       "      <th>ADV_spacy</th>\n",
       "      <th>PRON_spacy</th>\n",
       "      <th>DET_spacy</th>\n",
       "      <th>ADP_spacy</th>\n",
       "      <th>CONJ_spacy</th>\n",
       "      <th>total_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "      <td>The CDC currently reports deaths In general th...</td>\n",
       "      <td>[cdc, currently, report, death, general, discr...</td>\n",
       "      <td>{'DET': 3, 'PROPN': 1, 'ADV': 2, 'VERB': 2, 'N...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "      <td>States reported deaths a small rise from last ...</td>\n",
       "      <td>[state, report, death, small, rise, tuesday, s...</td>\n",
       "      <td>{'NOUN': 5, 'VERB': 2, 'DET': 2, 'ADJ': 3, 'AD...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "      <td>Politically Correct Woman Almost Uses Pandemic...</td>\n",
       "      <td>[politically, correct, woman, use, pandemic, e...</td>\n",
       "      <td>{'ADV': 2, 'ADJ': 1, 'PROPN': 7, 'ADP': 1, 'PA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "      <td>IndiaFightsCorona We have COVID testing labora...</td>\n",
       "      <td>[covid, testing, laboratory, india, august, test]</td>\n",
       "      <td>{'PROPN': 8, 'PRON': 1, 'AUX': 3, 'VERB': 2, '...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>[populous, state, generate, large, case, count...</td>\n",
       "      <td>{'ADJ': 5, 'NOUN': 7, 'AUX': 2, 'VERB': 3, 'CC...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label  \\\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real   \n",
       "1   2  States reported 1121 deaths a small rise from ...  real   \n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake   \n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real   \n",
       "4   5  Populous states can generate large case counts...  real   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  The CDC currently reports deaths In general th...   \n",
       "1  States reported deaths a small rise from last ...   \n",
       "2  Politically Correct Woman Almost Uses Pandemic...   \n",
       "3  IndiaFightsCorona We have COVID testing labora...   \n",
       "4  Populous states can generate large case counts...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [cdc, currently, report, death, general, discr...   \n",
       "1  [state, report, death, small, rise, tuesday, s...   \n",
       "2  [politically, correct, woman, use, pandemic, e...   \n",
       "3  [covid, testing, laboratory, india, august, test]   \n",
       "4  [populous, state, generate, large, case, count...   \n",
       "\n",
       "                                    spacy_pos_counts  NOUN_spacy  VERB_spacy  \\\n",
       "0  {'DET': 3, 'PROPN': 1, 'ADV': 2, 'VERB': 2, 'N...           9           2   \n",
       "1  {'NOUN': 5, 'VERB': 2, 'DET': 2, 'ADJ': 3, 'AD...           5           2   \n",
       "2  {'ADV': 2, 'ADJ': 1, 'PROPN': 7, 'ADP': 1, 'PA...           1           1   \n",
       "3  {'PROPN': 8, 'PRON': 1, 'AUX': 3, 'VERB': 2, '...           3           2   \n",
       "4  {'ADJ': 5, 'NOUN': 7, 'AUX': 2, 'VERB': 3, 'CC...           7           3   \n",
       "\n",
       "   ADJ_spacy  ADV_spacy  PRON_spacy  DET_spacy  ADP_spacy  CONJ_spacy  \\\n",
       "0          4          2           0          3          4           0   \n",
       "1          3          0           0          2          2           0   \n",
       "2          1          2           0          0          1           0   \n",
       "3          0          0           1          0          3           0   \n",
       "4          5          0           1          1          4           0   \n",
       "\n",
       "   total_spacy  \n",
       "0           24  \n",
       "1           14  \n",
       "2            6  \n",
       "3            9  \n",
       "4           21  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05469e53",
   "metadata": {},
   "source": [
    "From the values obtained from the POS columns we could infeer what is the context of the tweet(opinion or informative one) for example, the ones with more NOUNs and VERBs probably are informatives ones and the tweets tha have more ADJs and ADVs are probably opinion ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f38b5",
   "metadata": {},
   "source": [
    "## BERTopic Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de450e2",
   "metadata": {},
   "source": [
    "### Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "164b1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train['label'].map({'fake': 0, 'real': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9928753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['tweet_tokens']\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edc699ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
